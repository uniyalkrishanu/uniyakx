import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.types import *
import json
import boto3
from pyspark.sql.functions import to_date,count
import re
from awsglue import DynamicFrame
from pyspark.sql.types import *
import csv
from datetime import datetime
import copy
from boto3  import client as boto3_client
from datetime import datetime, timedelta
import dateutil
import os
import math
from pyspark.sql.functions import lit,sha2,concat_ws,col,concat,format_number,regexp_replace,when,trim
import pytz
from pytz import timezone
import pandas as pd

utc = pytz.UTC

#CONFIGURATIONS and other parameters
ssm = boto3.client('ssm', region_name="ap-south-1")
jobfrequency = 120
'''
s3BucketName = ssm.get_parameter(Name=processing_pipeline_params_namespace + '.targetbucket')['Parameter']['Value']
targetbucket = ssm.get_parameter(Name=processing_pipeline_params_namespace + '.publicationbucket')['Parameter']['Value'] #"aegon-publication-datalake-store-dt"
auroralambda = ssm.get_parameter(Name=processing_pipeline_params_namespace + '.aurora-lambda')['Parameter']['Value']
'''
s3BucketName = 'aegon-raw-datalake-store-qa'
targetbucket = 'deduptestbucket'
sourcepath = "s3://" + s3BucketName + "/"
targetpath = "s3://" + targetbucket + "/"
ERROR_DESCRIPTION = 'error_description'
targetstaging = "default_staging"
s3 = boto3.resource('s3', region_name="ap-south-1")
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

dynamodb = boto3.resource('dynamodb', region_name="ap-south-1")
table = dynamodb.Table('entitySearch')

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

JOB_NAME = args['JOB_NAME']

#hardcoded variable values
last_processed_time = '2019-10-07 10:00:00'
end_process_time = '2019-10-07 12:00:00'
entity_run_config_path = '{"hm-policy": "True","hm-payment":"True"}'

glueContext.clearCache()    

s3_bucket = s3.Bucket(targetbucket)  
raw_bucket = s3.Bucket(s3BucketName)

class Configuration:
    def __init__(self, entity_run_config_path):
        self.entity_run_config_path = entity_run_config_path
        self.run_config = None
        self.entities = None
    def fetch_run_config(self):
        ssm = boto3.client('ssm', region_name="ap-south-1")
        self.run_config = '{"hm-policy": "True","hm-payment": "True","hm-party": "True"}'
        self.run_config = json.loads(self.run_config)
    def return_entities(self):
        self.entities = [entity for entity, ind in self.run_config.items() if self.run_config[entity] == "True"]
        return self.entities
    def __str__(self):
        return json.dumps(self.run_config)

config = Configuration(entity_run_config_path)
config.fetch_run_config()
entities = config.return_entities()

def load_params(queries):
    entitynames = {}
    for query in queries:
        publicationentity = None
        if "publicationclass" in query:
            publicationentity = query["publicationclass"]
        # now query is a dictionary
        entitynames[query["resultClass"]] = [query["inputentities"],query["sql"],query["optionalfields"],publicationentity]
        
    return entitynames

def processEntity(targetstaging,sourceentities,targetentity,sqlQuery,seperator,parameters,optionalfields,outputformat,raw_bucket, source_folder):
    errordesc = ''
    datasourceDynamicOrig = {}
    fieldnotfound = []
    df_temp = {'created_at': [''],'policy_number': [''],'s3Key':[''], 'id':['']}
    df_temp = pd.DataFrame(df_temp)
    df_payload = {'created_at': [''],'policy_number': [''],'s3Key':[''], 'id':['']}
    df_payload = pd.DataFrame(df_payload)
    try:        
        selectQuery = "select"
        for ent in sourceentities:
            datasourceDynamicOrig[ent] = glueContext.create_dynamic_frame_from_options("s3", {'paths': [sourcepath + "/" + ent + "/year=2019/month=10/"], 'recurse':True, 'groupFiles': 'inPartition', "mergeSchema" : "true"}, format="json")
            dynamicdfCopy = datasourceDynamicOrig[ent]
            try:
                datasourceDynamicOrig[ent] = ResolveChoice.apply(datasourceDynamicOrig[ent], choice = "project:string")
                dfschema = datasourceDynamicOrig[ent].toDF().schema.json()
                dynamicdfCopy = datasourceDynamicOrig[ent]
            except:
                datasourceDynamicOrig[ent] = dynamicdfCopy
            pass
            datasourceDynamicOrig[ent].toDF().createOrReplaceTempView(source_folder.replace("-","_"))
            for key in parameters:
                glueContext.spark_session.sql("set " + key + "=" +  parameters[key] )
            for object in raw_bucket.objects.filter(Prefix=(source_folder+'/')):
                s3Key = object.key
                last_modified = object.last_modified.strftime('%Y-%m-%d %H:%M:%S')
                if not s3Key.endswith('/'):
                    if object.last_modified > datetime.strptime(last_processed_time, '%Y-%m-%d %H:%M:%S').replace(tzinfo=utc) and object.last_modified < datetime.strptime(end_process_time, '%Y-%m-%d %H:%M:%S').replace(tzinfo=utc):
                        for line in object.get()['Body']._raw_stream:
                            payload = json.loads(line.decode('utf-8').strip())
                            created_at = payload.get('created_at')
                            df_temp['created_at'] = created_at
                            if source_folder == 'hm-party':
                                id = payload.get('id')
                                policy_number = '000'
                            elif source_folder == 'hm-payment':
                                id = '000'
                                policy_number = payload.get('bill').get('policyNumber')                          
                            else:
                                id = '000'
                                policy_number = payload.get('policyNumber')
                            df_temp['policy_number'] = policy_number
                            df_temp['s3Key'] = s3Key
                            df_temp['id'] = id
                            df_payload = df_payload.append(df_temp)
            mySchema = StructType([ StructField("created_at", StringType(), True)\
                        ,StructField("policy_number", StringType(), True)\
                        ,StructField("s3_key", StringType(), True)\
                        ,StructField("id", StringType(), True)])				   
            df_payload = spark.createDataFrame(df_payload,schema=mySchema)
            #df_payload = df_payload.filter(df_payload.policy_number.isNotNull())
            df_payload = df_payload.filter(df_payload.policy_number != '')
            print("sqlQuery=" +str(sqlQuery))
            target_df = glueContext.spark_session.sql(sqlQuery).repartition(1).drop_duplicates()
            for field in fieldnotfound:
                target_df = target_df.withColumn(field, lit(None).cast(StringType()))
            df_payload = df_payload.withColumnRenamed("created_at","created_at1")
            df_payload = df_payload.withColumnRenamed("id","id1")
            if source_folder == 'hm-party':
                df_payload = df_payload.join(target_df, (df_payload.id1 == target_df.id) & (target_df.created_at == df_payload.created_at1), how='inner')
                #df_payload.printSchema()
                df_payload = df_payload.withColumn('policy_number',col('policyNumber'))
            else:
                df_payload = df_payload.join(target_df, (df_payload.policy_number == target_df.policyNumber) & (target_df.created_at == df_payload.created_at1), how='left')
            df_payload = df_payload.drop(*['policyNumber','created_at1','id','id1'])
            #target_df = target_df.drop("created_at1")
            df_payload = df_payload.coalesce(1)
            df_payload = df_payload.repartition(1)
            target_dynamicframe= DynamicFrame.fromDF(df_payload, glueContext, targetentity + "_dynamicframe")
            target_dynamicframe.show()
            passed_record_count = target_dynamicframe.count()
            payload_write = target_dynamicframe.toDF().drop_duplicates().toJSON().map(lambda j:json.loads(j)).collect()
            print('passed_record_count is:', passed_record_count)
            
            if passed_record_count > 0:
                target_dynamicframe.show()
                if outputformat == 'csv':
                    glueContext.write_dynamic_frame.from_options(frame = target_dynamicframe, connection_type = "s3", connection_options = {"path": targetpath + targetstaging + '/' + targetentity}, format = outputformat, format_options={"separator":seperator})
                    for drow in payload_write:
                        table.put_item(Item=drow)
    except Exception as e:
        print("targetentity =" + str(targetentity))
        exc_type, exc_obj, exc_tb = sys.exc_info()
        print("errordesc1=",exc_type, "line no" , exc_tb.tb_lineno)        
        errordesc = str(e)
        print("error_desc2=",errordesc,"error_description_Ends")

try:
    for object in s3_bucket.objects.filter(Prefix=("config")):
        if '.json' in object.key:
            print("object.key" + str(object.key))
            configpath = object.key
            
            obj = s3.Object(targetbucket, configpath)
            body= obj.get()['Body'].read().decode("utf-8")
                
            # parse x:
            parsed_json = json.loads(body)
            
            targetbucket = parsed_json["output"]["outputbucket"]
            targetstaging = parsed_json["output"]["outputfolder"]
            outputformat = parsed_json["output"]["outputformat"]
            
            publicationfolder = None
            if "publicationfolder" in parsed_json["output"]:
                publicationfolder = parsed_json["output"]["publicationfolder"]
            parameters = {}
            if "parameters" in parsed_json:
                for key in parsed_json["parameters"]:
                    parameters[key] = parsed_json["parameters"][key]
            seperator = ","
            if "seperator" in parsed_json["output"]:
                seperator = parsed_json["output"]["seperator"]
            entitynames = load_params(parsed_json["queries"])
            #publish a day before as per discussion with Mayank
            publicationdate = (datetime.now(timezone('Asia/Kolkata'))- timedelta(days = 1)).strftime("%y%m%d%H%M%S")
            for targetentity,values in entitynames.items():
                if targetentity in entities:
                    print(values[0])
                    #print(values[0].split(','))
                    sourceentities = values[0].split(',')
                    processEntity(targetstaging,values[0].split(','),targetentity,values[1],seperator,parameters,values[2],outputformat,raw_bucket,values[0])
except Exception as e:
    exc_type, exc_obj, exc_tb = sys.exc_info()
    print("errordesc1=",exc_type, "line no" , exc_tb.tb_lineno)        
    errordesc = str(e)
    print("errordesc=" + str(errordesc))
#processEntity({"policy","party"},"testpolicy",sqlQuery)
