import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.types import *
import json
import boto3
from pyspark.sql.functions import to_date,count
import re
from awsglue import DynamicFrame
from pyspark.sql.types import *
import csv
from datetime import datetime
import copy
from boto3  import client as boto3_client
from datetime import datetime, timedelta
import dateutil
import os
import math
from pyspark.sql.functions import lit,sha2,concat_ws,col,concat,format_number,regexp_replace,when,trim
import pytz
from pytz import timezone
import pandas as pd

utc = pytz.UTC

#CONFIGURATIONS and other parameters

args = getResolvedOptions(sys.argv, ['ENTITY_RUN_CONFIG',
                                     'ENTITY_PROCESSING_CONFIG',
                                     'SOURCE_BUCKET'
                                     ])

entity_run_config_path = args['ENTITY_RUN_CONFIG']
entity_processing_config_path = args['ENTITY_PROCESSING_CONFIG']

ssm = boto3.client('ssm', region_name="ap-south-1")
jobfrequency = 120
'''
s3BucketName = ssm.get_parameter(Name=processing_pipeline_params_namespace + '.targetbucket')['Parameter']['Value']
targetbucket = ssm.get_parameter(Name=processing_pipeline_params_namespace + '.publicationbucket')['Parameter']['Value'] #"aegon-publication-datalake-store-dt"
'''
s3BucketName = 'aegon-raw-datalake-store-qa'
targetbucket = 'deduptestbucket'

sourcepath = "s3://" + s3BucketName + "/"
targetpath = "s3://" + targetbucket + "/"
ERROR_DESCRIPTION = 'error_description'
targetstaging = "default_staging"
s3 = boto3.resource('s3', region_name="ap-south-1")

#dynamo connections
dynamodb = boto3.resource('dynamodb', region_name="ap-south-1")
table = dynamodb.Table('entitySearch-Policy')

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

#hardcoded variable values
last_processed_time = '2019-10-07 10:00:00'
entity_run_config_path = '{"hm-policy": "True","hm-payment":"False"}'

glueContext.clearCache()    

s3_bucket = s3.Bucket(targetbucket)  
raw_bucket = s3.Bucket(s3BucketName)

class Configuration:
    def __init__(self, entity_run_config_path):
        self.entity_run_config_path = entity_run_config_path
        self.run_config = None
        self.entities = None
    def fetch_run_config(self):
        ssm = boto3.client('ssm', region_name="ap-south-1")
        self.run_config = '{"hm-policy": "True","hm-payment": "False","hm-party": "False"}'
        self.run_config = json.loads(self.run_config)
    def return_entities(self):
        self.entities = [entity for entity, ind in self.run_config.items() if self.run_config[entity] == "True"]
        return self.entities
    def __str__(self):
        return json.dumps(self.run_config)

config = Configuration(entity_run_config_path)
config.fetch_run_config()
entities = config.return_entities()


class EntityIndexBuilder:
    entity_processing_config_path = args['ENTITY_PROCESSING_CONFIG']
    def __init__(self, entity):
        self.entity = entity
        self.entity_config = None
    def fetch_entity_conf(self):
        ssm = boto3.client('ssm', region_name="ap-south-1")
        self.entity_config = ssm.get_parameter(Name=self.entity_processing_config_path+self.entity)['Parameter']['Value']
        print(self.entity_config)
    def get_checkpoint(self):
        return '2020-01-01 00:00:00'
    def get_source_path(self):
        return json.loads(self.entity_config).get('sourceFolder')
    def get_table(self):
        table = dynamodb.Table('entitySearch-Policy')
        return table


def processEntity(targetstaging,sourceentities,targetentity, separator, outputformat,raw_bucket, source_folder, last_processed_time, table_name):
    errordesc = ''
    datasourceDynamicOrig = {}
    fieldnotfound = []
    df_temp = {'created_at': [''],'policy_number': [''],'s3Key':['']}
    df_temp = pd.DataFrame(df_temp)
    df_payload = {'created_at': [''],'policy_number': [''],'s3Key':['']}
    df_payload = pd.DataFrame(df_payload)
    try:        
        selectQuery = "select"
        for ent in sourceentities:
            print("entx:"+ent)
            datasourceDynamicOrig[ent] = glueContext.create_dynamic_frame_from_options("s3", {'paths': [sourcepath + "/" + ent + "/year=2020/"], 'recurse':True, 'groupFiles': 'inPartition', "mergeSchema" : "true"}, format="json")
            dynamicdfCopy = datasourceDynamicOrig[ent]
            try:
                datasourceDynamicOrig[ent] = ResolveChoice.apply(datasourceDynamicOrig[ent], choice = "project:string")
                dfschema = datasourceDynamicOrig[ent].toDF().schema.json()
                dynamicdfCopy = datasourceDynamicOrig[ent]
            except:
                datasourceDynamicOrig[ent] = dynamicdfCopy
            pass
            datasourceDynamicOrig[ent].toDF().createOrReplaceTempView(source_folder.replace("-","_"))
            for object in raw_bucket.objects.filter(Prefix=(source_folder+'/')):
                s3Key = object.key
                last_modified = object.last_modified.strftime('%Y-%m-%d %H:%M:%S')
                print("print outer loop")
                if not s3Key.endswith('/'):
                    if object.last_modified <= datetime.strptime(last_processed_time, '%Y-%m-%d %H:%M:%S').replace(tzinfo=utc):
                        for line in object.get()['Body']._raw_stream:
                            payload = json.loads(line.decode('utf-8').strip())
                            created_at = payload.get('created_at')
                            df_temp['created_at'] = created_at
                            if source_folder == 'hm-policy':
                                policy_number = payload.get('policyNumber')
                            df_temp['policy_number'] = policy_number
                            df_temp['s3Key'] = s3Key
                            df_payload = df_payload.append(df_temp)
                            print("inner loop")
                            print(s3Key)
            mySchema = StructType([ StructField("created_at", StringType(), True)\
                        ,StructField("policy_number", StringType(), True)\
                        ,StructField("s3_key", StringType(), True)])				   
            df_payload = spark.createDataFrame(df_payload,schema=mySchema)
            #df_payload = df_payload.filter(df_payload.policy_number.isNotNull())
            df_payload = df_payload.filter(df_payload.policy_number != '')
            passed_record_count = df_payload.count()
            target_dynamicframe = DynamicFrame.fromDF(df_payload, glueContext, "entitysearch_dynamicframe")
            payload_write = target_dynamicframe.toDF().drop_duplicates().toJSON().map(lambda j:json.loads(j)).collect()
            print('passed_record_count is:', passed_record_count)
            if passed_record_count > 0:
                target_dynamicframe.show()
                if outputformat == 'csv':
                    glueContext.write_dynamic_frame.from_options(frame = target_dynamicframe, connection_type = "s3", connection_options = {"path": targetpath + targetstaging + '/' + targetentity}, format = outputformat, format_options={"separator":separator})
                    for drow in payload_write:
                        with table.batch_writer() as batch:
                            batch.put_item(Item=drow)
    except Exception as e:
        print("targetentity =" + str(targetentity))
        exc_type, exc_obj, exc_tb = sys.exc_info()
        print("errordesc1=",exc_type, "line no" , exc_tb.tb_lineno)        
        errordesc = str(e)
        print("error_desc2=",errordesc,"error_description_Ends")

for entity in entities:
    entity_obj = EntityIndexBuilder(entity)
    entity_obj.fetch_entity_conf()
    source_folder = entity_obj.get_source_path()
    table_name = entity_obj.get_table()
    last_processed_time = entity_obj.get_checkpoint()
    separator = ","
    sourceentities = [source_folder]
    targetentity = source_folder
    outputformat = "csv"
    processEntity(targetstaging,sourceentities,targetentity, separator, outputformat,raw_bucket, source_folder, last_processed_time, table_name)
